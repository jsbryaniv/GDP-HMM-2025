
########################################
        4/15/25
########################################


Yesterday I compared 3 things: 
1) Data augmentation
2) Using a concatentation block instead of summing the skip connections
3) Using a different value of alpha, where alpha is the decay factor for the skip connections in each convolutional block.

These are the results:
job0 -- Augment=True,  Catblock=True,  Alpha=1.0
job1 -- Augment=True,  Catblock=False, Alpha=1.0
job2 -- Augment=True,  Catblock=True,  Alpha=0.5
job3 -- Augment=True,  Catblock=False, Alpha=0.5
job4 -- Augment=False, Catblock=True,  Alpha=1.0
job5 -- Augment=False, Catblock=False, Alpha=1.0
job6 -- Augment=False, Catblock=True,  Alpha=0.5
job7 -- Augment=False, Catblock=False, Alpha=0.5
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.2041
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.2663
outfiles/logs/out_job2.txt:-- Average loss on test dataset: 3.5315
outfiles/logs/out_job3.txt:-- Average loss on test dataset: 3.3507
outfiles/logs/out_job4.txt:-- Average loss on test dataset: 3.3231
outfiles/logs/out_job5.txt:-- Average loss on test dataset: 3.3166
outfiles/logs/out_job6.txt:-- Average loss on test dataset: 3.7731
outfiles/logs/out_job7.txt:-- Average loss on test dataset: 3.6574

Keep in mind that the first four jobs trained longer than the last four jobs. Also I am just showing the final ouput, but in reality there were some differences between the way these models trained. Also the second four had fewer features (16 vs 64) so I was testing that as well.

I still dont see the MAE going below 3 which is concerning. I thought that this set I was doing exactly what I was before when I got MAE=2.7, but I guess not.

########################################
        4/15/25
########################################

Yesterday I ran two tests comparing concatentation blocks vs summing different levels of the Unet.
```
            # Merge with skip
            x_skip = feats.pop()               # Get skip connection
            if self.use_catblock:
                x = torch.cat([x, x_skip], dim=1)  # Concatenate features
                x = self.cat_blocks[i](x)                    # Apply convolutional layers
            else:
                x = x + x_skip  # Add skip connection
```

Today I looked at the results:

Key:
    job0 -- With catblock=True
    job1 -- With catblock=False

outfiles/logs/out_job0.txt:-- Average loss on test dataset: 8.0269
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 5.6686
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 4.7342
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 4.7342
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 4.0648
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 4.0012
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 4.0012
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.8625
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.9142
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.6924
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.6924
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.6924
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.6924
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.5108
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.5108
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.5108
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.5108
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.2922
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.2922
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.2922
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.2084
outfiles/logs/out_job0.txt:-- Average loss on test dataset: 3.2084
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 5.5403
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 4.9865
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 4.9865
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.9980
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.9980
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.9980
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.8038
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.7355
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.6474
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.6474
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.6474
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.5804
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.5430
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.5430
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.5430
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.5430
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4612
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4612
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4612
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.3433
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.3433
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4834
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4834
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4834
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4834
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4834
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.4834
outfiles/logs/out_job1.txt:-- Average loss on test dataset: 3.1827

The catblock does not seem to be helping much, however reverting to summing by itself did not reduce the error to the previous 2.61 MAE. I will need to find what other modifications I made since 3/24 that could be causing this.


